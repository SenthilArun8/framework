
[2025-12-12 19:17:30.412449] Graph Execution Failed: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
Traceback (most recent call last):
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2958, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 5230, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 4012, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1388, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1222, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 376, in iter
    result = action(retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1201, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\senth\Downloads\framework\main.py", line 142, in process_turn
    output = self.app.invoke(inputs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 3068, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 2643, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\senth\Downloads\framework\src\brain.py", line 150, in subconscious_node
    thought = chain.invoke({
        "mood": profile.current_mood,
    ...<4 lines>...
        "context_instruction": context_instruction
    })
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\runnables\base.py", line 3143, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2461, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 398, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 927, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2962, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
During task with name 'subconscious' and id '651745ba-251d-3dad-b4f0-aeaaa0636081'


[2025-12-12 19:18:03.945740] Graph Execution Failed: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
Traceback (most recent call last):
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2958, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 5230, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 4012, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1388, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1222, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 376, in iter
    result = action(retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1201, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\senth\Downloads\framework\main.py", line 142, in process_turn
    output = self.app.invoke(inputs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 3068, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 2643, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\senth\Downloads\framework\src\brain.py", line 150, in subconscious_node
    thought = chain.invoke({
        "mood": profile.current_mood,
    ...<4 lines>...
        "context_instruction": context_instruction
    })
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\runnables\base.py", line 3143, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2461, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 398, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 927, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2962, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
During task with name 'subconscious' and id '67d787f0-b4b0-eac6-0282-d7a5da606a40'


[2025-12-12 19:18:28.825401] Graph Execution Failed: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
Traceback (most recent call last):
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2958, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 5230, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 4012, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1388, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1222, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 376, in iter
    result = action(retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1201, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\senth\Downloads\framework\main.py", line 142, in process_turn
    output = self.app.invoke(inputs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 3068, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 2643, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\senth\Downloads\framework\src\brain.py", line 150, in subconscious_node
    thought = chain.invoke({
        "mood": profile.current_mood,
    ...<4 lines>...
        "context_instruction": context_instruction
    })
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\runnables\base.py", line 3143, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2461, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 398, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 927, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2962, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
During task with name 'subconscious' and id 'a7aa5073-8bd6-a017-d182-2857cc4c1103'


[2025-12-12 19:19:38.113896] Graph Execution Failed: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
Traceback (most recent call last):
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2958, in _generate
    response: GenerateContentResponse = self.client.models.generate_content(
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 5230, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\models.py", line 4012, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1388, in request
    response = self._request(http_request, http_options, stream=False)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1222, in _request
    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 376, in iter
    result = action(retry_state)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\tenacity\__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\_api_client.py", line 1201, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 121, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\google\genai\errors.py", line 146, in raise_error
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\senth\Downloads\framework\main.py", line 142, in process_turn
    output = self.app.invoke(inputs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 3068, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\main.py", line 2643, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langgraph\_internal\_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "C:\Users\senth\Downloads\framework\src\brain.py", line 150, in subconscious_node
    thought = chain.invoke({
        "mood": profile.current_mood,
    ...<4 lines>...
        "context_instruction": context_instruction
    })
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\runnables\base.py", line 3143, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2461, in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 398, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 927, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_core\language_models\chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 2962, in _generate
    _handle_client_error(e, request)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\senth\AppData\Roaming\Python\Python313\site-packages\langchain_google_genai\chat_models.py", line 145, in _handle_client_error
    raise ChatGoogleGenerativeAIError(msg) from e
langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}
During task with name 'subconscious' and id '9c46fbc9-4b68-690f-9251-73a51508da2a'

